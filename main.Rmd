---
title: "Mine or Rock?"
author: "Reza Dwi Utomo"
date: "24/02/2020"
output:
  html_document:
    highlight: zenburn
    number_sections: yes
    theme: flatly
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction {#intro}

This article aims to accomplish [Classification in Machine Learning 1](https://algorit.ma/course/classification-1/) course at Algoritma. The dataset used is obtained from [the University of California at Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)), "Connectionist Bench (Sonar, Mines vs. Rocks)". You could see the source code fully in my GitHub account [here](https://github.com/utomoreza/C1_LBB).

## Aim

The goal is to model red wine quality based on physicochemical tests.

## Objectives

1. To solve the final model equation

2. To output the statistical values (adjusted) R-squared, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and MAE (Mean Absolute Error)

3. To examine the model including statistics and visualizations:

+ Assess linearity of model (parameters)
+ Assess serial independence of errors
+ Assess heteroscedasticity
+ Assess normality of residual distribution
+ Assess multicollinearity

4. To interpretate the model

5. To consider other factors, such as:

+ Are there any outliers?
+ Are there missing values?

6. To test the model using dataset test and discuss the results

## Structure

This article is arranged as follows.

1. [Introduction](#intro)
2. [Metadata](#meta)
3. [Preparation](#prep)
4. [Exploratory Data Analysis](#eda)
5. [Modelling](#model)
6. [Model Improvements](#modimprov)
7. [Results and Discussions](#resdis)
8. [Conclusions](#conc)

# Metadata {#meta}

## Content

The task is to train a network to discriminate between sonar signals bounced off a metal cylinder and those bounced off a roughly cylindrical rock. The file "sonar.mines" contains 111 patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. The file "sonar.rocks" contains 97 patterns obtained from rocks under similar conditions. The transmitted sonar signal is a frequency-modulated chirp, rising in frequency. The data set contains signals obtained from a variety of different aspect angles, spanning 90 degrees for the cylinder and 180 degrees for the rock.

Each pattern is a set of 60 numbers in the range 0.0 to 1.0. Each number represents the energy within a particular frequency band, integrated over a certain period of time. The integration aperture for higher frequencies occur later in time, since these frequencies are transmitted later during the chirp.

The label associated with each record contains the letter "R" if the object is a rock and "M" if it is a mine (metal cylinder). The numbers in the labels are in increasing order of aspect angle, but they do not encode the angle directly.

## Source

* Relevant Paper

Gorman, R. P., and Sejnowski, T. J. (1988). "Analysis of Hidden Units in a Layered Network Trained to Classify Sonar Targets" in Neural Networks, Vol. 1, pp. 75-89.
[Web Link]


# Preparation {#prep}

```{r message=FALSE, echo=FALSE}
library(tidyverse) # for data wrangling
library(plotly) # for plotting using plotly style
library(ggcorrplot) # for plotting correlation
library(caret) # for confusion matrix
library(gtools) # for converting log of odds to probs
library(PerformanceAnalytics) # for pair plotting
library(car) # for executing VIF test
library(rsample) # for splitting dataset into train and test with controlled proportion
library(class) # for KNN
library(MLmetrics) # for calculating accuracy
```

```{r}
sonar <- read.csv("sonar.all-data", header = F)
head(sonar)
```

```{r}
colnames(sonar) <- c(paste0("energy",c(1:60)), "type")
head(sonar)
```

# Data Wrangling

```{r}
anyNA(sonar)
```

```{r}
str(sonar)
```

```{r}
summary(sonar)
```

```{r warning=FALSE}

p <- sonar %>% select(colnames(.)[-61]) %>%
  pivot_longer(cols = colnames(.), names_to = "Energy", values_to = "Value") %>%
  arrange(Energy) %>% 
  mutate(Energy = as.factor(Energy)) %>%
  ggplot(aes(x = Energy,
             y = Value)) +
  coord_flip() +
  geom_boxplot(aes(fill = Energy), show.legend = F) +
  theme(axis.text.y = element_blank(),
        axis.ticks.y = element_blank(),
        legend.position = "none")
ggplotly(p)
```

# Exploratory Data Analysis

## Target Variable Proportion

```{r}
table(sonar$type)
prop.table(table(sonar$type))
```

## Multicollinearity

```{r}
temp <- sonar %>% select(-type)
colnames(temp) <- c(1:60)
ggcorrplot::ggcorrplot(temp %>% cor())


GGally::ggcorr(temp)
# ggcorrplot::ggcorrplot(sonar %>% select(-type) %>% cor(),
#   hc.order = TRUE, 
#   type = "lower",
#   lab = TRUE,
#   digits = 1,
#   ggtheme = ggplot2::theme_dark(),
# )

# GGally::ggpairs(DF %>% select(-RainTomorrow))
# pairs.panels(DF %>% select(-RainTomorrow))
```

There is a unique pattern discovered from above figure. Each variable has large positive correlation with their next neighbors. Therefore, we have to remove this pattern by dropping the adjacent neighbors of each variable. We can carry out this by only take the columns with multiplication of 3, 4, or 5. As the beginning, I pick the multiplication of 4 to divide the number of predictor variables.

```{r}
idx <- c(1, seq(from = 4, to = ncol(temp), by = 4))
GGally::ggcorr(temp[,idx], label = T)
DF <- sonar[,c(idx,61)] # subset only necessary columns, i.e. the multipl of 4
```

Nice. We have removed all with high correlation. Now, we should check the p-value of correlation test.

```{r}
sonarComb <- combn(colnames(DF[,-61]), 2)
Alpha <- 0.05
multicolRes <- data.frame(vs = 1:dim(sonarComb)[2], 
                          cor = 1:dim(sonarComb)[2],
                          res = 1:dim(sonarComb)[2])
for (i in 1:dim(sonarComb)[2]) {
  multicolRes$vs[i] <- paste0(sonarComb[1,i], " & ", sonarComb[2,i])
  corTest <- cor.test(DF[,sonarComb[1,i]], DF[,sonarComb[2,i]])
  multicolRes$cor[i] <- corTest$p.value
  multicolRes$res[i] <- ifelse(corTest$p.value < Alpha, 
                               "Yes", 
                               "No")
  # multiResults <- c(multiResults, res)
}
multicolRes
```

```{r}
table(multicolRes$res)
```

```{r}
multicolRes[multicolRes$res == "No",]
```

Surprisingly, all predictor variables indicate multicollinearity to each other. But, since we have no other variables except the ones provided, we can keep them all.

# Modelling and Predictions

## Splitting

```{r}
set.seed(1)
idx <- initial_split(DF, prop = 0.8, strata = type)
sonar_train <- training(idx)
sonar_test <- testing(idx)

prop.table(table(sonar_train$type)) # Check train dataset proportion after split
prop.table(table(sonar_test$type)) # Check test dataset proportion after split

# Split the predictors and the target of train dataset for KNN model usage
X_train <- sonar_train[,-ncol(sonar_train)]
y_train <- sonar_train[,ncol(sonar_train)]

# Split the predictors and the target of test dataset for KNN model usage
X_test <- sonar_test[,-ncol(sonar_test)]
y_test <- sonar_test[,ncol(sonar_test)]
```

```{r}
set.seed(1)
ranIdx <- sample(nrow(sonar_train), nrow(sonar_train))
sonar_train <- sonar_train[ranIdx,]
rownames(sonar_train) <- NULL
```

```{r}
set.seed(2)
ranIdx <- sample(nrow(sonar_test), nrow(sonar_test))
sonar_test <- sonar_test[ranIdx,]
rownames(sonar_test) <- NULL
```

## Create the Model

### Logistic Regression Model

```{r}
model_log <- glm(formula = type ~ ., data = sonar_train, family = "binomial", maxit = 30)
summary(model_log)
```

### KNN Data Pre-Processing - Scalling

```{r}
X_train.scaled <- scale(x = X_train)
X_test.scaled <- scale(x = X_test, 
                       center = attr(X_train.scaled, "scaled:center"),
                       scale = attr(X_train.scaled, "scaled:scale"))
```

## Predictions

### Logistic Regression Prediction

```{r}
predict_log <- predict(object = model_log, newdata = sonar_test, type = "response")
sonar_test$ypred_prob <- predict_log
sonar_test$ypred_label <- ifelse(sonar_test$ypred_prob > 0.5, "R", "M")
# negative class is M
# positive class is R
# since at the beginning, R automatically sets "M" as the first as class, meaning the negative class
```

### KNN Prediction

* Find optimum `K`

```{r}
K <- sqrt(nrow(X_train))
K
```

As the target variable has two classes only (`R` or `M`) meaning that it is an even number, we need an odd number of `K`. Therefore, with `K = 13`, it is enough for the prediction.

```{r}
predict_knn <- knn(train = X_train.scaled, test = X_test.scaled, cl = y_train, k = 13)
```

# Evaluation

## Confusion Matrix
```{r}
print("Confusion Matrix of Log Regression Model")
confusionMatrix(data = as.factor(sonar_test$ypred_label), reference = y_test, positive = "R")
print("Confusion Matrix of KNN")
confusionMatrix(data = predict_knn, reference = y_test, positive = "R")
```

## ROC/AUC

```{r}
library(ROCR)
options(scipen = 999)
df_ <- data.frame("prediction" = predict_log, 
                      "trueclass" = as.numeric(y_test == "R"))

df__roc <- prediction(df_$prediction, df_$trueclass)
perf <- performance(df__roc, "tpr", "fpr")
rocDF <- data.frame(threshold = perf@alpha.values[[1]],
                    tpr = perf@y.values[[1]],
                    fpr = perf@x.values[[1]])

ggplot(rocDF, aes(x = fpr, y = tpr)) +
  geom_line(color = "blue") +
  geom_point(color = "black") +
  # geom_roc(n.cuts=20,labels=FALSE) +
  # style_roc(theme = theme_grey) +
  geom_abline(color = "grey", intercept = 0, slope = 1, linetype="dashed") +
  labs(main = "Receiver operating characteristic curve",
       x = "False positive rate",
       y = "True positive rate")
```

```{r}
# show AUC value
auc <- ROCR::performance(prediction.obj = df__roc, "auc")
auc@y.values[[1]]
```

# Model Tuning

## Tuning Logistic Regression

```{r}
energyInsignificant <- c("energy8", "energy28", "energy32", "energy40", "energy60")
sonar_train2 <- sonar_train %>% select(-all_of(energyInsignificant))
```

```{r}
model_log2 <- glm(formula = type ~ ., data = sonar_train2, family = "binomial", maxit = 30)
summary(model_log2)
```

```{r}
predict_log2 <- predict(object = model_log2, newdata = sonar_test, type = "response")
sonar_test$ypred_prob2 <- predict_log2
sonar_test$ypred_label2 <- ifelse(sonar_test$ypred_prob2 > 0.5, "R", "M")
```

```{r}
confusionMatrix(as.factor(sonar_test$ypred_label2), sonar_test$type, positive = "R")
```

Tuning threshold

```{r}
options(scipen = 999)
df_ <- data.frame("prediction" = predict_log2, 
                      "trueclass" = as.numeric(y_test == "R"))

df__roc <- prediction(df_$prediction, df_$trueclass)
perf <- performance(df__roc, "tpr", "fpr")
rocDF <- data.frame(threshold = perf@alpha.values[[1]],
                    tpr = perf@y.values[[1]],
                    fpr = perf@x.values[[1]])

ggplot(rocDF, aes(x = fpr, y = tpr)) +
  geom_line(color = "blue") +
  geom_point(color = "black") +
  # geom_roc(n.cuts=20,labels=FALSE) +
  # style_roc(theme = theme_grey) +
  geom_abline(color = "grey", intercept = 0, slope = 1, linetype="dashed") +
  labs(main = "Receiver operating characteristic curve",
       x = "False positive rate",
       y = "True positive rate")
```

```{r}
rocDF$threshold[1] <- 1
rocDF$spec <- 1 - rocDF$fpr # find specificity value by 1 - false positive rate
rocDF[which.max(rocDF$tpr + rocDF$spec),] # get the best threshold by finding maximum true positive rate and maximum specificity
optThreshold <- rocDF[which.max(rocDF$tpr + rocDF$spec),1]
```

```{r}
# show AUC value
auc <- ROCR::performance(prediction.obj = df__roc, "auc")
auc@y.values[[1]]
```

```{r}
sonar_test$ypred_prob2b <- predict_log2
sonar_test$ypred_label2b <- ifelse(sonar_test$ypred_prob2b > optThreshold, "R", "M")
confusionMatrix(as.factor(sonar_test$ypred_label2b), sonar_test$type, positive = "R")
```

```{r}
Accuracy(y_pred = sonar_test$ypred_label, y_true = sonar_test$type)
Accuracy(y_pred = sonar_test$ypred_label2, y_true = sonar_test$type)
Accuracy(y_pred = sonar_test$ypred_label2b, y_true = sonar_test$type)
```

```{r}
Recall(y_pred = sonar_test$ypred_label, y_true = sonar_test$type, positive = "R")
Recall(y_pred = sonar_test$ypred_label2, y_true = sonar_test$type, positive = "R")
Recall(y_pred = sonar_test$ypred_label2b, y_true = sonar_test$type, positive = "R")
```

```{r}
Precision(y_pred = sonar_test$ypred_label, y_true = sonar_test$type, positive = "R")
Precision(y_pred = sonar_test$ypred_label2, y_true = sonar_test$type, positive = "R")
Precision(y_pred = sonar_test$ypred_label2b, y_true = sonar_test$type, positive = "R")
```

## Tuning KNN

### Find best K

```{r}
oddK_idx <- sapply(1:(nrow(X_train)-1), function(x) {ifelse(x%%2 == 1, T, F)})
optK <- data.frame(1:(nrow(X_train)-1))[oddK_idx,]
optKNN <- data.frame(K = 1:length(optK), acc = 1:length(optK))

for (i in 1:length(optK)) {
  predknn <- knn(train = X_train.scaled, test = X_test.scaled, cl = y_train, k = optK[i])
  optKNN$K[i] <- optK[i]
  optKNN$acc[i] <- Accuracy(predknn, y_test)
}
maxAcc <- optKNN[optKNN$acc == max(optKNN$acc),]
label <- paste0(round(maxAcc$acc, 2), " when K = ", maxAcc$K)
optKNN %>% ggplot(aes(x = K,
                      y = acc)) +
  geom_line(color = "blue") +
  geom_point(data = maxAcc, aes(x = K,
                                y = acc)) +
  geom_label(data = maxAcc, aes(label = label), position = position_dodge(20), label.size = 0.05) +
  labs(title = "No of K vs Accuracy in KNN prediction",
       x = "Number of Ks",
       y = "Accuracy")
```

```{r}
bestK <- maxAcc$K
predict_knn2 <- knn(train = X_train.scaled, test = X_test.scaled, cl = y_train, k = bestK)
confusionMatrix(predict_knn2, reference = y_test, positive = "R")
```

```{r}
Accuracy(y_pred = predict_knn, y_true = y_test)
Accuracy(y_pred = predict_knn2, y_true = y_test)
```

```{r}
Recall(y_pred = predict_knn, y_true = y_test, positive = "R")
Recall(y_pred = predict_knn2, y_true = y_test, positive = "R")
```

```{r}
Precision(y_pred = predict_knn, y_true = y_test, positive = "R")
Precision(y_pred = predict_knn2, y_true = y_test, positive = "R")
```


















Prepare the performance indicators and all necessary functions.

```{r warning=FALSE, message=FALSE}
library(MLmetrics)
indicator <- function(model, y_pred, y_true) {
     adj.r.sq <- summary(model)$adj.r.squared
     mse <- MSE(y_pred, y_true)
     rmse <- RMSE(y_pred, y_true)
     mae <- MAE(y_pred, y_true)
     print(paste0("Adjusted R-squared: ", round(adj.r.sq, 4)))
     print(paste0("MSE: ", round(mse, 4)))
     print(paste0("RMSE: ", round(rmse, 4)))
     print(paste0("MAE: ", round(mae, 4)))
}

metrics <- function(y_pred, y_true){
     mse <- MSE(y_pred, y_true)
     rmse <- RMSE(y_pred, y_true)
     mae <- MAE(y_pred, y_true)
     print(paste0("MSE: ", round(mse, 6)))
     print(paste0("RMSE: ", round(rmse, 6)))
     print(paste0("MAE: ", round(mae, 6)))
     corPredAct <- cor(y_pred, y_true)
     print(paste0("Correlation: ", round(corPredAct, 6)))
     print(paste0("R^2 between y_pred & y_true: ", round(corPredAct^2, 6)))
}

CheckNormal <- function(model) {
     hist(model$residuals, breaks = 30)
     shaptest <- shapiro.test(model$residuals)
     print(shaptest)
     if (shaptest$p.value <= 0.05) {
          print("H0 rejected: the residuals are NOT distributed normally")
     } else {
          print("H0 failed to reject: the residuals ARE distributed normally")
     }
}

library(lmtest)
CheckHomos <- function(model){
     plot(model$fitted.values, model$residuals)
     abline(h = 0, col = "red")
     BP <- bptest(model)
     print(BP)
     if (BP$p.value <= 0.05) {
          print("H0 rejected: Error variance spreads INCONSTANTLY/generating patterns (Heteroscedasticity)")
     } else {
          print("H0 failed to reject: Error variance spreads CONSTANTLY (Homoscedasticity)")
     }
}
```

```{r}
redDF <- read.csv("winequality-red.csv", sep = ";")
redDF
```

```{r warning=FALSE, message=FALSE}
library(tidyverse)

redDF %>% is.na() %>% colSums()
```

Perfect. Since the used datasets are originally clean, we will not find any missing value. So, let's move on to explore the data.

# Exploratory Data Analysis {#eda}

In order to explore the dataset, we could use scatter plots, histograms, correlation value, and p-value.

```{r warning=FALSE, message=FALSE}
# library(psych)
# pairs.panels(redDF)

# library(ggcorrplot)
# ggcorrplot(redDF %>% cor(),
#   hc.order = TRUE, type = "lower",
#   lab = TRUE,
#   digits = 1,
#   ggtheme = ggplot2::theme_dark(),
# )

library(PerformanceAnalytics)
chart.Correlation(redDF, hist = T)
```

The preceding figure tells many things. But, in general, it shows four points: scatter plots between each variable, histograms of each variable, correlation values between each value, and p-values between each value against significance value of 0.05.

## Scatter plots {#scat}

Surprisingly, we found something interesting here. The scatter plots of between `quality` and each predictor variable form the same pattern that the target variable `quality` classifies the values into several classess, i.e. 3, 4, 5, 6, 7, and 8. To examine this case, we will go through to assess linear regression to model the data.

Moreover, there are several predictors which have strong relationship, e.g. between `fixed.acidity` and `citric.acid`. They are indicated by their tendency to have inclined or declined line. This case is discussed further in the Correlation values point below.

## Histograms {#hist}
     
Each predictor variable shows values distributed appropriately. However, the target variable exhibits poor distribution. This supports the above finding from scatter plots analysis. We could check the summary of such variable to make sure.

```{r collapse=TRUE}
summary(redDF$quality)
table(redDF$quality)
```

Unsuprisingly, `quality` does have classified values. **Based on this finding, it seems that linear regression is not suitable for this dataset. This is our initial hypothesis.**

## Correlation values {#corr}

The figure above shows that below relationships have a strong correlation.

 + Between `density` and `fixed.acidity` (0.67)
 + Between `fixed.acidity` and `citric.acid` (0.67)
 + Between `fixed.acidity` and `pH` (-0.68)
 + Between `free.sulfur.dioxide` and `total.sulfur.dioxide` (0.67)
     
Those perhaps indicate sufficiently high multicollinearity. We will highlight this issue and discuss it later in the [assumptions](#asum) section.

## P-values {#pval}

In addition, it is only `volatile.acidity` and `alcohol` which have the largest correlation value with `quality`. However, we also need to check the Pearson's correlation test based on the p-value. As seen in the figure above, the red stars in the upper triangle of the matrix indicate the significance. The more the stars exist, the more significant the relationship is. In order to be significant enough to the significance value (we use significance value (alpha) of 0.05), we need at least one star.

In this p-value analysis, we're only interested in considering the p-values of relationship between `quality` and each predictor variable. We can see that all variables have at least one star (meaning p-value less than pre-determined alpha (i.e. 0.05)), except `residual.sugar`. So, we won't consider such variable any longer.

# Modelling {#model}

## Splitting Train Datasets and Test Datasets

By using the dataset, we're going to split it up into 80% of data for train datasets and 20% of data for test datasets.

```{r}
set.seed(1)
sampleSize <- round(nrow(redDF)*0.8)
idx <- sample(seq_len(sampleSize), size = sampleSize)

X.train_red <- redDF[idx,]
X.test_red <- redDF[-idx,]

rownames(X.train_red) <- NULL
rownames(X.test_red) <- NULL
```

## Create the Model

As mentioned in the [exploratory data analysis](#pval), we will employ all predictor variables, except `residual.sugar`, for the model. Let's create linear model from those variables.

```{r}
model_red1 <- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, 
                 data = X.train_red)
summary(model_red1)
```

From the `summary()` function above, it can be seen that approximately a half number of all predictor variables exhibits insignificance. Furthermore, the adjusted R-squared also performs poor result. Before we tackle with this issue, we should check the assumptions of the model.

## Check the assumptions {#asum}

Since the linearity assumption has been discussed earlier in [this section](#corr), here, we're going to use three assumptions, i.e. normality, homoscedasticity, and multicollinearity.

### Normality
     
By employing normality assumption, we'd like to have the residuals of the predicted value to approach the normal distribution. We can check this by plotting the residuals and using Shapiro-Wilk normality test. For the latter, we expect to have p-value more than significance value (i.e. 0.05) so that the null hypothesis is failed to reject.

In the [Preparation](#prep) chapter, we have declared a function to carry out this task called `CheckNormal()`. So, let's use it.
     
```{r}
CheckNormal(model = model_red1)
```

Although it seems the figure above indicates that the residuals tend to gather around 0 number (i.e. approaching to have normal distribution), we are unable to immediately believe this. We also have to check the results of Shapiro-Wilk normality test. And unfortunately, in the case of normality, our model shows poor results. The p-value is so small that H0 is rejected, meaning that the residuals is **not** distributed normally. We don't want this.

### Homoscedasticity
 
In homoscedasticity aspect, we'd like to have residuals spreading constantly randomly, without generating any pattern. We have two approaches to examine this aspect, i.e. plotting the residuals vs the predicted values and performing the Breusch-Pagan test. As the function `CheckHomos()` to carry out this task has been declared already in [Preparation](#prep), we just need to use it.

```{r}
CheckHomos(model = model_red1)
```

As read above, the p-value is so small that null hypothesis is rejected. Moreover, the figure above also points out line-like patterns. This indeed states that the residuals generate patterns, meaning that heteroscedasticity exists. We don't want this.

### Multicollinearity
 
In multicollinearity factor, inside the model, we'd like to have each predictor variable **not** demonstrating strong relationship with each other. We could examine this factor by inspecting their VIF (Variance Inflation Factor) score. We expect to have VIF score not greater than 10. We can perform this task by using the function `vif()` from the `car` package.

```{r warning=FALSE, message=FALSE}
library(car)
vif(model_red1)
```

By reading their score above, we see that the only largest value is `fixed.acidity`, i.e. ± 6.9. Fortunately, such score is still lower than 10. Therefore, in case of multicollinearity, our model performs satisfactorily.

# Model Improvements {#modimprov}

As stated in the previous chapter, by using `summary()` function and checking the assumptions, the model performs poor results, except in case of multicollinearity. Thus, any improvement has to be executed to decrease its drawbacks.

## Check the Outliers

Firstly, let's check outliers of the dataset whether any high leverage high influence exist. We could use four plots here, i.e. Residuals vs Fitted, Normal Q-Q, Cook's Distance, and Residuals vs Leverage. For your information regarding those plots, you could read [here](https://data.library.virginia.edu/diagnostic-plots/).

```{r}
par(mfrow=c(2,2)) # Change the panel layout to 2 x 2 
lapply(c(1,2,4,5), # showing 4 types of plots
       function(x) plot(model_red1, 
                        which = x, 
                        # labels.id = 1:nrow(X.train_red),
                        cook.levels = c(0.05, 0.1))) %>% invisible()
```

From four figures above, we found there are some leverages with high influence, i.e. the observations with index 78, 202, 245, 274, and 1161. We're going to remove those rows.

```{r}
to.rm <- c(78,202,245,274,1161)
# X.train_red[to.rm,]
X.train_red <- X.train_red[-to.rm,]
rownames(X.train_red) <- NULL
```

After the outliers removed, a new model is generated, and also check its summary. 

```{r}
model_red2 <- lm(quality ~ fixed.acidity + volatile.acidity + citric.acid + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + density + pH + sulphates + alcohol, 
                 data = X.train_red)
summary(model_red2)
```

It seems the new model performs more reliable. To make sure, let's check the adjusted R-squared values between two models.

```{r collapse=TRUE}
print("Adjusted R-squared for 1st model:")
ad.r.sq1 <- summary(model_red1)$adj.r.squared
ad.r.sq1
print("Adjusted R-squared for 2nd model:")
ad.r.sq2 <- summary(model_red2)$adj.r.squared
ad.r.sq2
print(paste0("The difference between both is ", round(ad.r.sq2-ad.r.sq1, 5)*100, "%"))
```

Well done. Adjusted R-squared increases by almost 2%. Now, we move on to try feature selection to improve the model.

## Feature Selection Implementation

We're going to employ step-wise algorithm for the feature selection method. We will use three directions of the algorithm, i.e. backward, forward, and both. First of all, we have to define the models for lower and upper threshold of the algorithm.

### Create two models as threshold for the step wise algorithm
     
```{r}
model_redAlc <- lm(quality ~ alcohol, data = X.train_red)
# summary(model_redAlc)
model_redAll <- lm(quality ~ ., data = X.train_red)
# summary(model_redAll)
```

Now, let's carry out three approaches of step-wise algorithm.

### Backward approach

```{r}
step(model_redAll, direction = "backward", trace = F)
```

```{r}
model.back_red <- lm(formula = quality ~ volatile.acidity + chlorides + free.sulfur.dioxide + 
    total.sulfur.dioxide + pH + sulphates + alcohol, data = X.train_red)
summary(model.back_red)
```

### Forward approach

```{r}
step(model_redAlc, scope = list(lower = model_redAlc, upper = model_redAll),
     direction = "forward",
     trace = F)
```

```{r}
model.forw_red <- lm(formula = quality ~ alcohol + volatile.acidity + sulphates + total.sulfur.dioxide + chlorides + pH + free.sulfur.dioxide, 
    data = X.train_red)
summary(model.forw_red)
```

### Both approach
  
```{r}
step(model_redAlc, scope = list(lower = model_redAlc, upper = model_redAll),
     direction = "both",
     trace = F)
```

```{r}
model.both_red <- lm(formula = quality ~ alcohol + volatile.acidity + sulphates + total.sulfur.dioxide + chlorides + pH + free.sulfur.dioxide, 
                     data = X.train_red)
summary(model.both_red)
```

All three approaches have been defined. Now, we're going to compare our all models so far by their adjusted R-squared.

```{r collapse=TRUE}
cat("Adjusted R-squared for 1st model:\n")
ad.r.sq1 <- summary(model_red1)$adj.r.squared
ad.r.sq1
cat("\nAdjusted R-squared for 2nd model:\n")
ad.r.sq2 <- summary(model_red2)$adj.r.squared
ad.r.sq2
cat("\nAdjusted R-squared for model using 'alcohol' variable only:\n")
ad.r.sqAlc <- summary(model_redAlc)$adj.r.squared
ad.r.sqAlc
cat("\nAdjusted R-squared for model using all variables:\n")
ad.r.sqAll <- summary(model_redAll)$adj.r.squared
ad.r.sqAll
cat("\nAdjusted R-squared for model with backward approach:\n")
ad.r.sqBack <- summary(model.back_red)$adj.r.squared
ad.r.sqBack
cat("\nAdjusted R-squared for model with forward approach:\n")
ad.r.sqForw <- summary(model.forw_red)$adj.r.squared
ad.r.sqForw
cat("\nAdjusted R-squared for model with both approach:\n")
ad.r.sqBoth <- summary(model.both_red)$adj.r.squared
ad.r.sqBoth
```

Evidently, after we have performed feature selection, we don't obtain the model with much higher performance. Instead, the best model so far is achieved not from such selection, but from manually including all available predictor variables. **Hence, from now on, the best model used will be the one with all predictor variables, i.e. `model_redAll`**

# Results and Discussions {#resdis}

In this chapter, we're going to discuss the best model so far and use it to predict the test dataset. Firstly, we should interpret the selected model. Subsequently, the performance of the model is discussed and the predictions will be carried out later.

## Model Interpretation

The selected model is the one with all available preditor variables. We defined it as `model_redAll`. It consist of the following equation:

$\hat{Y} = \beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\beta_6X_6+\beta_7X_7+\beta_8X_8+\beta_9X_9+\beta_{10} X_{10}+\beta_{11}X_{11}$ 

where the following are values from the $\beta_0$ to $\beta_{11}$ and from $X_1$ to $X_{11}$:

```{r}
model_redAll$coefficients
```

From the equation above, we can interpret that the line starts from the Cartesian coordinate of (0, 37.87), as pointed by the intercept. Furthermore, along with the increase of any $X_i$, the related $\beta_i$ will adjust the line according to both values.

For example and for simplicity, if we were to have $X_{alcohol} = 1$, then the y coordinate (or so-called the predicted value) would be:
$\hat{Y} = \beta_0 + \beta_{alcohol}.X_{alcohol} = 37.87 + 0.27*1 = 38.14$

## Check the Performances

Here, we're going to check the performances of the chosen model. The metrics used are Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE). We just need the `indicator()` function defined in the beginning.

```{r collapse=TRUE}
indicator(model = model_redAll, y_pred = model_redAll$fitted.values, y_true = X.train_red$quality)
```

With performances as shown above, we will compare them to those of the prediction using test dataset.

## Predictions

Here, we're going to compare the performances of prediction using train dataset to those of using test dataset. The metrics used are MSE, RMSE, MAE, correlation between `y_pred` and `y_true`, and R-squared between `y_pred` and `y_true`. The plots between `y_pred` and `y_true` for each train dataset and test dataset also will be shown.

```{r collapse=TRUE}
cat("Performance using train dataset:\n")
metrics(y_pred = model_redAll$fitted.values, y_true = X.train_red$quality)

redPredict.back <- predict(model_redAll, newdata = X.test_red)
cat("\nPerformances using test dataset:\n")
metrics(y_pred = redPredict.back, y_true = X.test_red$quality)
```
```{r}
redFitted.back <- data.frame(qualityPred = model.back_red$fitted.values,
                                qualityAct = X.train_red$quality)
ggplot(redFitted.back, aes(x = qualityPred,
                       y = qualityAct)) +
     geom_point(aes(color = as.factor(qualityAct)), show.legend = F) +
     geom_smooth(method = "lm", se = F) +
     labs(title = "Predicted vs Actual Values Using Train Dataset",
          x = "Predicted quality",
          y = "Actual quality")
```
```{r}
redPredict.backDF <- data.frame(qualityPred = redPredict.back,
                                qualityAct = X.test_red$quality)
ggplot(redPredict.backDF, aes(x = qualityPred,
                       y = qualityAct)) +
     geom_point(aes(color = as.factor(qualityAct)), show.legend = F) +
     geom_smooth(method = "lm", se = F) +
     labs(title = "Predicted vs Actual Values Using Test Dataset",
          x = "Predicted quality",
          y = "Actual quality")
```

There are several points we can infer from the performance results above:

1. The model overfits the train dataset so that it performs poor when using test dataset.
2. As the model has defective results, it is unable to satisfactorily predict the target variable.
3. The plots verify poin number 2 that the model in fact is ineffective to predict the target variable. 

# Conclusions {#conc}

We have finished this article. Below are the points we can conclude from this article:

* A linear model has been created. The target variable is `quality`, whereas the attributes of physicochemical tests are as the predictor variables.

* The selected model is the one with all available variables. So, the model equation is as follows:

$\hat{Y} = \beta_0+\beta_1X_1+\beta_2X_2+\beta_3X_3+\beta_4X_4+\beta_5X_5+\beta_6X_6+\beta_7X_7+\beta_8X_8+\beta_9X_9+\beta_{10} X_{10}+\beta_{11}X_{11}$ 

* The statistical values (adjusted) R-squared, Mean Squared Error (MSE), Root Mean Squared Error (RMSE), and Mean Absolute Error (MAE) of the selected model have been calculated.

```{r}
indicator(model = model_redAll, y_pred = model_redAll$fitted.values, y_true = X.train_red$quality)
```

* The selected model has been examined and improved using statistics tests (the assumptions and feature selection) and visualizations in [Modelling](#model) and [Model Improvements](#modimprov) chapters.

* The selected model has been interpretated in [Results and Discussions](#resdis) chapter.

* The selected model has been tested using test dataset test and discussed in [Results and Discussions](#resdis) chapter.

* The selected model performs ineffective in modelling the train dataset. The best adjusted R-squared value produced is only at 0.3832.

* As the selected model show poor performances, it also demonstrates deficient results when predicting the test dataset.

* As stated earlier in [histogram](#hist) and [scatter plots](#scat) sections that it is found an initial hypothesis that the target variable has classified values instead of continuous values so it seems the linear regression is not suitable with the dataset, such hypothesis has been proven. All results and discussions in this article verify it.

* As mentioned above, therefore, it can be concluded that for the type of this dataset, **in particular a target variable with classified values, it is not recommended to model the data using linear regression.**

* For future study, other algorithms will be applied to model this wine quality dataset.